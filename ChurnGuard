# -*- coding: utf-8 -*-
"""CS4375(Final Project Demo).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gydkt6TmZpgmebmoilieYNDGA1vv6ZUn
"""

import numpy as np
class DecisionTreeClassifier:
    def __init__(self, max_depth=None):
        self.max_depth = max_depth

    def fit(self, X, y):
        self.n_classes_ = len(set(y))
        self.n_features_ = X.shape[1]
        self.tree_ = self._grow_tree(X, y)

    def _grow_tree(self, X, y, depth=0):
        num_samples_per_class = [np.sum(y == i) for i in range(self.n_classes_)]
        predicted_class = np.argmax(num_samples_per_class)
        node = {'predicted_class': predicted_class}

        if self.max_depth is not None and depth >= self.max_depth:
            return node

        if len(set(y)) == 1:
            return node

        feature_index, threshold = self._find_best_split(X, y)
        if feature_index is None:
            return node

        indices_left = X[:, feature_index] < threshold
        X_left, y_left = X[indices_left], y[indices_left]
        X_right, y_right = X[~indices_left], y[~indices_left]
        node['feature_index'] = feature_index
        node['threshold'] = threshold
        node['left'] = self._grow_tree(X_left, y_left, depth + 1)
        node['right'] = self._grow_tree(X_right, y_right, depth + 1)
        return node

    def _find_best_split(self, X, y):
        m = y.size
        if m <= 1:
            return None, None
        num_parent = [np.sum(y == c) for c in range(self.n_classes_)]
        best_gini = 1.0 - sum((n / m) ** 2 for n in num_parent)
        best_feature, best_threshold = None, None

        for feature_index in range(self.n_features_):
            thresholds, classes = zip(*sorted(zip(X[:, feature_index], y)))
            num_left = [0] * self.n_classes_
            num_right = num_parent.copy()

            for i in range(1, m):
                c = classes[i - 1]
                num_left[c] += 1
                num_right[c] -= 1
                gini_left = 1.0 - sum(
                    (num_left[x] / i) ** 2 for x in range(self.n_classes_)
                )
                gini_right = 1.0 - sum(
                    (num_right[x] / (m - i)) ** 2 for x in range(self.n_classes_)
                )
                gini = (i * gini_left + (m - i) * gini_right) / m

                if thresholds[i] == thresholds[i - 1]:
                    continue
                if gini < best_gini:
                    best_gini = gini
                    best_feature = feature_index
                    best_threshold = (thresholds[i] + thresholds[i - 1]) / 2
        return best_feature, best_threshold

    def _predict(self, inputs):
        node = self.tree_
        while 'threshold' in node:
            if inputs[node['feature_index']] < node['threshold']:
                node = node['left']
            else:
                node = node['right']
        return node['predicted_class']

    def predict(self, X):
        return np.array([self._predict(inputs) for inputs in X])

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder

# Load the dataset
data = pd.read_csv("WA_Fn-UseC_-Telco-Customer-Churn (1).csv")

# Drop unnecessary columns if needed
# data.drop(columns=['customerID'], inplace=True)

# Data preprocessing
# Convert categorical columns to numeric using LabelEncoder
cat_columns = data.select_dtypes(include=['object']).columns
for column in cat_columns:
    data[column] = LabelEncoder().fit_transform(data[column])

# Splitting features and target variable
X = data.drop(columns=['Churn'])
y = data['Churn']

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Training the Decision Tree model
model = DecisionTreeClassifier(max_depth=5)
model.fit(X_train_scaled, y_train)

# Making predictions
y_pred = model.predict(X_test_scaled)

# Evaluating the model
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred)

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)
print("ROC-AUC:", roc_auc)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv("WA_Fn-UseC_-Telco-Customer-Churn (1).csv")

# Summary statistics for numerical features
summary_statistics = data.describe()
print("Summary Statistics:")
print(summary_statistics)

cat_columns = data.select_dtypes(include=['object']).columns
for column in cat_columns:
    data[column] = LabelEncoder().fit_transform(data[column])
# Visualizations
# Histograms for numerical features
numerical_features = ['tenure', 'MonthlyCharges', 'TotalCharges']
for feature in numerical_features:
    plt.figure(figsize=(8, 6))
    sns.histplot(data[feature], bins=20, kde=True)
    plt.title(f'Histogram of {feature}')
    plt.xlabel(feature)
    plt.ylabel('Frequency')
    plt.show()

# Box plot for TotalCharges
plt.figure(figsize=(8, 6))
sns.boxplot(x='Churn', y='TotalCharges', data=data)
plt.title('Box Plot of TotalCharges by Churn Status')
plt.xlabel('Churn')
plt.ylabel('TotalCharges')
plt.show()

# Correlation matrix
correlation_matrix = data.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()

import numpy as np

class GradientBoostingClassifier:
    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):
        self.n_estimators = n_estimators
        self.learning_rate = learning_rate
        self.max_depth = max_depth
        self.trees = []
        self.f0 = None

    def fit(self, X, y):
        self.f0 = np.mean(y)
        f = np.full_like(y, self.f0)

        for _ in range(self.n_estimators):
            residual = y - f
            tree = DecisionTreeClassifier(max_depth=self.max_depth)
            tree.fit(X, residual)
            self.trees.append(tree)
            f = (f + self.learning_rate * tree.predict(X)).astype(y.dtype)

    def predict(self, X):
        f = np.full(X.shape[0], self.f0)
        for tree in self.trees:
            f += self.learning_rate * tree.predict(X)
        return np.round(f)

from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

# Data preprocessing
data = pd.read_csv("WA_Fn-UseC_-Telco-Customer-Churn (1).csv")

# Convert categorical columns to numeric using LabelEncoder
cat_columns = data.select_dtypes(include=['object']).columns
for column in cat_columns:
    data[column] = LabelEncoder().fit_transform(data[column])

# Splitting features and target variable
X = data.drop(columns=['Churn'])
y = data['Churn']

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Training the Gradient Boosting model
model = GradientBoostingClassifier(n_estimators=10, learning_rate=0.1, max_depth=3)
model.fit(X_train_scaled, y_train)

# Making predictions
y_pred = model.predict(X_test_scaled)

# Evaluating the model
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred)

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)
print("ROC-AUC:", roc_auc)

import numpy as np

class RandomForestClassifier:
    def __init__(self, n_estimators=100, max_depth=None, max_features=None, random_state=None):
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.max_features = max_features
        self.random_state = random_state
        self.trees = []

    def fit(self, X, y):
        np.random.seed(self.random_state)
        for _ in range(self.n_estimators):
            tree = DecisionTreeClassifier(max_depth=self.max_depth)
            sample_indices = np.random.choice(len(X), size=len(X), replace=True)
            tree.fit(X[sample_indices], y[sample_indices])
            self.trees.append(tree)

    def predict(self, X):
        predictions = np.zeros((X.shape[0], len(self.trees)))
        for i, tree in enumerate(self.trees):
            predictions[:, i] = tree.predict(X)
        return np.round(np.mean(predictions, axis=1))

# Load the dataset
import pandas as pd
from sklearn.preprocessing import LabelEncoder

data = pd.read_csv("WA_Fn-UseC_-Telco-Customer-Churn (1).csv")

# Data preprocessing
cat_columns = data.select_dtypes(include=['object']).columns
for column in cat_columns:
    data[column] = LabelEncoder().fit_transform(data[column])
# Splitting features and target variable
X = data.drop(columns=['Churn'])
y = data['Churn']

# Splitting the dataset into training and testing sets
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
X_train.reset_index(drop=True, inplace=True)
y_train.reset_index(drop=True, inplace=True)
# Training the Random Forest model
model = RandomForestClassifier(n_estimators=10, max_depth=None, max_features='auto', random_state=42)
model.fit(X_train_scaled, y_train)

# Making predictions
y_pred = model.predict(X_test_scaled)

# Evaluating the model
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred)

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)
print("ROC-AUC:", roc_auc)

import numpy as np

class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01, num_iterations=1000):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.learning_rate = learning_rate
        self.num_iterations = num_iterations

        # Initialize weights and biases
        self.W1 = np.random.randn(self.input_size, self.hidden_size)
        self.b1 = np.zeros((1, self.hidden_size))
        self.W2 = np.random.randn(self.hidden_size, self.output_size)
        self.b2 = np.zeros((1, self.output_size))

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def sigmoid_derivative(self, x):
        return x * (1 - x)

    def forward_propagation(self, X):
        # Forward pass through the network
        self.z1 = np.dot(X, self.W1) + self.b1
        self.a1 = self.sigmoid(self.z1)
        self.z2 = np.dot(self.a1, self.W2) + self.b2
        self.a2 = self.sigmoid(self.z2)

    def backward_propagation(self, X, y):
        # Backward pass through the network
        self.loss = y - self.a2
        self.dz2 = self.loss * self.sigmoid_derivative(self.a2)
        self.dW2 = np.dot(self.a1.T, self.dz2)
        self.db2 = np.sum(self.dz2, axis=0, keepdims=True)
        self.dz1 = np.dot(self.dz2, self.W2.T) * self.sigmoid_derivative(self.a1)
        self.dW1 = np.dot(X.T, self.dz1)
        self.db1 = np.sum(self.dz1, axis=0, keepdims=True)

    def update_parameters(self):
        # Update weights and biases
        self.W1 += self.learning_rate * self.dW1
        self.b1 += self.learning_rate * self.db1
        self.W2 += self.learning_rate * self.dW2
        self.b2 += self.learning_rate * self.db2

    def fit(self, X, y):
        for _ in range(self.num_iterations):
            # Forward propagation
            self.forward_propagation(X)

            # Backward propagation
            self.backward_propagation(X, y)

            # Update parameters
            self.update_parameters()

    def predict(self, X):
        # Make predictions
        self.forward_propagation(X)
        return np.round(self.a2)

import pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import numpy as np

# Read the dataset
data = pd.read_csv("WA_Fn-UseC_-Telco-Customer-Churn (1).csv")

# Convert categorical columns to numeric using LabelEncoder
cat_columns = data.select_dtypes(include=['object']).columns
for column in cat_columns:
    data[column] = LabelEncoder().fit_transform(data[column])

# Split features and target variable
X = data.drop(columns=['Churn'])
y = data['Churn']

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Reshape y_train to match the shape of the output layer
y_train = y_train.to_numpy().reshape(-1, 1)

# Initialize and train the neural network model
model = NeuralNetwork(input_size=X_train.shape[1], hidden_size=4, output_size=1, learning_rate=0.01, num_iterations=1000)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred)

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)
print("ROC-AUC:", roc_auc)

import numpy as np

class LogisticRegression:
    def __init__(self, learning_rate=0.01, num_iterations=1000):
        self.learning_rate = learning_rate
        self.num_iterations = num_iterations
        self.weights = None
        self.bias = None

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    def fit(self, X, y):
        # Initialize weights and bias
        self.weights = np.zeros((X.shape[1], 1))
        self.bias = 0

        # Convert y to numpy array and reshape
        y = np.array(y).reshape(-1, 1)

        # Gradient descent
        for _ in range(self.num_iterations):
            # Forward pass
            z = np.dot(X, self.weights) + self.bias
            a = self.sigmoid(z)

            # Compute gradients
            dz = a - y
            dw = np.dot(X.T, dz) / X.shape[0]
            db = np.mean(dz)

            # Update parameters
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db

    def predict(self, X):
        # Predict probabilities
        z = np.dot(X, self.weights) + self.bias
        a = self.sigmoid(z)

        # Convert probabilities to binary predictions
        return (a > 0.5).astype(int)

import pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Read the dataset
data = pd.read_csv("WA_Fn-UseC_-Telco-Customer-Churn (1).csv")

# Convert categorical columns to numeric using LabelEncoder
cat_columns = data.select_dtypes(include=['object']).columns
for column in cat_columns:
    data[column] = LabelEncoder().fit_transform(data[column])

# Split features and target variable
X = data.drop(columns=['Churn'])
y = data['Churn']

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Initialize and train the logistic regression model
model = LogisticRegression(learning_rate=0.01, num_iterations=1000)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred)

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)
print("ROC-AUC:", roc_auc)
